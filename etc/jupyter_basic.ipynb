{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01 : Setting (import Library)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02 : NumPy API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개요 : 머신러닝의 근본 / 행렬 배열 계산\n",
    "- 차원\n",
    "    - [ ] = axis 중첩수 : 차원수\n",
    "    - axis0 : 신규차원의 방향이 axis0  \n",
    "- 생성\n",
    "    - import numpy as np\n",
    "    - np.array( [ndarray] )\n",
    "    - np.arange( start = int , stop = int )\n",
    "    - np.zeros( int, int)\n",
    "    - np.ones( int, int)\n",
    "- 재구성\n",
    "    - A.reshape( axis0 size , axis1 size , axis2 size ... )\n",
    "        - -1 을 넣는 다면 타 입력값에 맞춰 적절한 값을 자동으로 적용해 준다\n",
    "- 인덱싱\n",
    "    - Sampling : array[ int , int ]\n",
    "    - Slicing : array[ 1:3 , : ]\n",
    "    - Fancy index : array[ [1,2,3], 1:3 ]\n",
    "        - 연속된 값의 경우 차원이 축소 되지 않는다\n",
    "    - Boolean index : array_answer = array[ array > 5 ]\n",
    "- 정렬\n",
    "    - np.sort( [ndarray] )\n",
    "        - 원 행렬은 유지한 채 재정렬된 행렬을 반환\n",
    "    - ndarray.sort()\n",
    "        - 원 행렬 자체를 지정한 형태로 재정렬 후 , 반환값은 None\n",
    "        - default : 오름차순 , 내림차순 ndarray.sort()[::-1]\n",
    "    - np.argsort( )\n",
    "- 선형대수 연산\n",
    "    - 내적 : np.dot( A , B)\n",
    "    - 전치 : np.transpose( A )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03 : Pandas API  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개요 : Python 데이터 처리에 있어 가장 인기있는 라이브러리\n",
    "- 구성\n",
    "    - Series : Index / DataFrame : Index & Column\n",
    "- 생성\n",
    "    - import pandas as pd\n",
    "    - pd.read_csv('파일명.csv')\n",
    "    - pd.DataFrame(dic_name)\n",
    "- DATA check\n",
    "    - A.head()\n",
    "    - A.tail()\n",
    "    - A.info()\n",
    "    - A.shape()\n",
    "    - A.describe()\n",
    "    - df_name['col_name'].value_count()\n",
    "        - dropna = True : default\n",
    "        - dropna = False : null 값도 count 를 진행한다\n",
    "- Type change\n",
    "    - List -> ndarray -> DF\n",
    "    - dict -> DF\n",
    "    - DF -> ndarray -> List\n",
    "    - DF -> dict\n",
    "- axis 추가\n",
    "    - A['new_col'] = data or A['col'] func\n",
    "- axis 수정\n",
    "    - A['org_col'] = data or A['col'] func\n",
    "- axis 삭제 : 지정axis ( col or row ) 삭제\n",
    "    - A.drop(['name_1', 'name_2',...], axis = , inplace = ]\n",
    "        - inplace = Flase (default)\n",
    "        - inplace = True\n",
    "- axis 재정렬\n",
    "    - A.reset_index([drop = , inplace = )\n",
    "- axis 재명명\n",
    "    - A.rename(columns = {'old_1' : 'new_1' , 'old_2' : 'new_2'}, inplace = ) \n",
    "- 필터링\n",
    "    - 기본식 : A['col_name']\n",
    "    - 명칭기반\n",
    "        - A['col'].loc[ , ]\n",
    "    - 위치기반\n",
    "        - A['col'].iloc[ , ]\n",
    "    - Boolean indexing\n",
    "        - A[ boolean ]\n",
    "- 정렬\n",
    "    - A.sort_values(by = 'col_name' , asending = , inplace = )\n",
    "- Aggregation (집합연산)\n",
    "    - .sum()\n",
    "    - .max()\n",
    "    - .min()\n",
    "    - .count()\n",
    "    - .mean()\n",
    "- GroupBy\n",
    "    - A.groupby(by='col_name').agg({ 'col_1' : [np.func_1, np.func_2], 'col_2' : np.func_3 })\n",
    "- Null data\n",
    "    - null check\n",
    "        - A['col'].isna()\n",
    "    - null fill\n",
    "        - A['col'].fillna('a')\n",
    "        - A['col'].replace(np.nan, 'new_data', inplace = )\n",
    "- 고유값 확인\n",
    "    - A['col'].value_counts() : 객체 고유값의 수량\n",
    "    - A['col'].nunique() : 객체 고유값의 종류 가짓 수\n",
    "- DF 변경\n",
    "    - A['col'].replace( { 'org_data' : 'new_data' }, inplace = )\n",
    "    - A['col'].replace(np.nan, 'new_data', inplace = )\n",
    "- lambda\n",
    "    - func = lambda x : return_value\n",
    "    - .apply(lambda x : return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 04 : Scikit-Learn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 개요 : 머신러닝 라이브러리 중 가장많이 사용되는 라이브러리\n",
    "\n",
    "- 학습의 종류 (Estimator)\n",
    "    - Supervised Learning (지도학습) : train / label set 으로 data set 분류\n",
    "        - Classification (분류)\n",
    "            - DecisionTree Classifier / RandomForest Classifier / \n",
    "        - Reggressor (회귀)\n",
    "            - Linear Regression\n",
    "    - Un-Supervised Learning (비지도학습)\n",
    "        - Clustering (군집)\n",
    "        - Visualization (시각화) & Dimension reduction : 차원축소\n",
    "        - Association Rule Learing : 연관규칙학습\n",
    "    - Semi-Supervised Learning (준지도학습)\n",
    "        - Deep Belief Network (DBN)\n",
    "        - Restricted Boltzmann Machine\n",
    "    - Reinforcement Learning (강화학습)\n",
    "        - 학습 시스템 : 에이전트\n",
    "            - 행동(action) 을 통해 보상(reward) 와 벌점(penalty) 를 받음\n",
    "            - 시간이 흘러가며 가장큰 보상(reward) 을 얻기위한 정책(policy) 만이 생존하는 전략으로 학습\n",
    "\n",
    "- ML 학습 프로세스\n",
    "    - 각 라이브러리 호출\n",
    "        - Scikit-learn / Estimator / Data-set (feature / target) / etc-modules \n",
    "    - 데이터 셋 분리\n",
    "    - ML 객체 생성\n",
    "    - 학습 및 검증 진행\n",
    "        > a.fit( X_train, y_train )\n",
    "    - 평가 및 정확도 측정\n",
    "        - 예측치 획득\n",
    "            > pred = a.predict( X_test )\n",
    "        - 정확도 측정\n",
    "            > score = accuracy_score( pred , y_test )\n",
    "\n",
    "- 학습/테스트 분리\n",
    "    - 임의 분리\n",
    "        > from sklearn.model_selection import train_test_split  \n",
    "        > X-train, X_Test, y_train, y_test = train_test_split( F_df, L_df, test_size = , random_state = )\n",
    "    - KFold\n",
    "        > from sklearn.model_selection import KFold  \n",
    "        > kfold = KFold(n_split = )  \n",
    "        > n = 0  \n",
    "        > for train_index, test_index in kfold.split(F_df) :  \n",
    "            >> n += 1  \n",
    "            >> X_train, X_test = F_df[train_index], F_df[test_index]  \n",
    "            >> y_train, y_test = L_df[train_index], L_df[test_index]\n",
    "    - Stratified KFold\n",
    "        > from sklearn.model_selection import StratifiedKFold  \n",
    "        > stfkfold = StratifiedKFold(n_split = )  \n",
    "        > n = 0  \n",
    "        > for train_index, test_index in stfkfold.split(F_df, L_df) :  \n",
    "            >> n += 1  \n",
    "            >> X_train, X_test = F_df[train_index], F_df[test_index]  \n",
    "            >> y_train, y_test = L_df[train_index], L_df[test_index]\n",
    "    - cross_val_score()\n",
    "        - Straitified 기반으로 분류 하여 평가\n",
    "        > from sklearn.model_selection import cross_val_score, cross_validate  \n",
    "        > score = scross_val_score( ML_API, F_df, L_df, scoring = 'accuracy', cv = )\n",
    "    - GridsearchCV : 하이퍼 파라미터 튜닝\n",
    "        > from sklearn.model_selection import GridSearchCV  \n",
    "        > parameters = {'max_depth' = [ ... ] , 'min_sample_split' = [ ... ] , 'min_sample_leaf' = [ ... ]}  \n",
    "        - 하이퍼 튜닝 평가  \n",
    "        > gird_A = GridSearchCV( ML_API , param = parameters , scoring = 'accuracy' , cv = )  \n",
    "\n",
    "        - 하이퍼 튜닝 결과 학습  \n",
    "        > grid_A = gird_A.best_estimator_  \n",
    "\n",
    "        - 하이퍼 튜닝 학습 ML 평가 및 정확도 측정  \n",
    "        > pred = grid_A.predict(X_test)  \n",
    "        > socre = accuracy_score(pred, y_test)\n",
    "        \n",
    "- 전처리\n",
    "    - 데이터 인코딩 : str -> int\n",
    "        - 레이블 인코딩\n",
    "            - 특징\n",
    "                - 글자수? 앒파벳순서? 따라 번호 부여\n",
    "                - 의도하지 않은 weight 발생 가능\n",
    "            - 호출\n",
    "            > from sklearn.preprocessing import LabelEncoder\n",
    "            - 객체 생성\n",
    "            > encoder = LabelEncoder( )\n",
    "            - 인코더 column 학습\n",
    "            > encoder.fit(F_df['col'])\n",
    "            - 인코딩\n",
    "            > encoder.transform(F_df['col'])\n",
    "        - 원핫 인코딩\n",
    "            - 특징\n",
    "                - 객체별 하나의 column 씩 분할\n",
    "            - 사용방법\n",
    "                - 입력 : ndarray ()\n",
    "                - 반환 : Sparse Matrix (희소행렬)\n",
    "                    - toarray 를 통해 Dense Matrix 형태로 재변환 후 사용해야함\n",
    "            - 장점 : 레이블 인코딩 weight 관련 문제점 보완\n",
    "            - 종류\n",
    "                - sklearn & numpy  활용\n",
    "                    > from sklearm.preprocessing import OneHotEncoder\n",
    "                    - 2차원 ndarray 생성\n",
    "                    > a = np.array(df['col']).reshape(-1,1)\n",
    "                    - 객체 생성\n",
    "                    > encoder = OneHotEncoder()  \n",
    "                    - 학습\n",
    "                    > encoder.fit(a)  \n",
    "                    - 인코딩\n",
    "                    > oh_labels = encoder.transform(a)\n",
    "                    - 출력\n",
    "                    > oh_labels.toarray()\n",
    "\n",
    "                - pd.get_dummies() 활용\n",
    "                    > df_oh = pd.dummies(df_org['col'])  \n",
    "                    > df_new = pd.concat([df_org, df_oh])  \n",
    "                    > df_new.drop('col', index = 1 , inplace = True )\n",
    "                    \n",
    "    - 스케일링\n",
    "        - 정의\n",
    "            - 서로다은 변수를 비교가능한 수준의 변수 값으로 맞추는 작업 (Feature Scaling)\n",
    "        - 종류\n",
    "            - 표준화 (Standardization)\n",
    "                - 평균 0 / 분산 1 : 가우시안 정규 분포\n",
    "                - StandarScaler\n",
    "                    > from sklearn.preprocessing import StandardScaler  \n",
    "                    > scaler = StandardScaler()  \n",
    "                    > scaler.fit(df_org)  \n",
    "                    > df_scaled = scaler.transform(df_org)\n",
    "            - 정규화 (Nomalization)\n",
    "                - 최소 0 / 최대 1 : 데이터 분포가 가우시안 분포가 아닐경우 적용\n",
    "                    - 서로다른 feature 의 크기를 통일\n",
    "                - MinMaxScaler\n",
    "                    > from sklearn.preprocessing import MinMaxScaler  \n",
    "                    > scaler = MinMaxScaler()  \n",
    "                    > scaler.fit(df_org)  \n",
    "                    > df_scaled = scaler.transform(df_org)\n",
    "        - 유의사항\n",
    "            - feature_df 와 label_df 의 data set 구성이 다를 수 있으므로\n",
    "            - 단 한번의 fit으로 활용한다\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 : Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 : Setting (Import Libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 data EDA Library\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random as rnd \n",
    "import missingno as msno\n",
    "import sklearn\n",
    "import xgboost\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 plot vizualize Library\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import seaborn as sns \n",
    "sns.set(font_scale=1.0) \n",
    "plt.style.use('seaborn-whitegrid') \n",
    "sns.set_theme(style='whitegrid', font_scale=1.0) \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 ML / DL Library\n",
    "import tensorflow as tf\n",
    "import keras as kr \n",
    "from keras import layers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 scikit-learn algorithm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC    \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 모델 튜닝 및 평가\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import model_selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 경고 제거 (판다스가 에어 메섹지를 자주 만들어 내기 때문)\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제목"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdc2b39dba4a1a2bb6d840bc8828727e3a4c4135026ee5b84640756028bbf810"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
